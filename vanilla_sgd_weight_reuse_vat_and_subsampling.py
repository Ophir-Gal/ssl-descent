# -*- coding: utf-8 -*-
"""Vanilla_SGD_weight_reuse_VAT_and_subsampling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-y_c4FvLME_st5oCAGIzQt7AV40chMki

# Virtual Adversarial Training 
Updated from the EEAT colab file from the google drive

*   Shadow Package: https://pypi.org/project/shadow-ssml/
*   Shadow Docs: https://shadow-ssml.readthedocs.io/en/latest/examples/mnist_example.html 
*   Shadow API: https://shadow-ssml.readthedocs.io/en/latest/documentation.html

Follows from this example: https://shadow-ssml.readthedocs.io/en/latest/examples/mnist_example.html

VAT original paper: https://arxiv.org/pdf/1507.00677.pdf
VAT summary in SSL paper, page 12 : https://arxiv.org/pdf/2006.05278.pdf

# Imports
"""

# torch imports
import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchsummary import summary

# shadow-ssml imports
!pip install git+https://github.com/sandialabs/shadow.git
import shadow.vat
from shadow.utils import set_seed

# helpers
import numpy as np
import random
import matplotlib.pyplot as plt
import math

from sklearn.model_selection import train_test_split
from torch.utils.data import TensorDataset, DataLoader

"""# Obtaining the Data
Instead of using the MNIST class for the fully-labeled training datasets, we define our own MNIST class to return partially labeled (labeled and unlabeled) training data. Then we define our dataset for training as the MNIST training data with 90% of the labels reassigned to a value to -1 (this will be used later to treat the data as unlabeled) using a consistent sampling seed. Lastly, we use the standard torchvision MNIST class test partition, keeping all labels, for evaluation of SSL classification performance.
"""

datadir = 'data'
set_seed(0)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load mnist test data from Colab. 
# we will use this data (10k) for our training and testing purposes. The train and test set would be disjoint images
subset = np.genfromtxt('./sample_data/mnist_test.csv', delimiter=',')
X, y = subset[:, 1:], subset[:, 0].astype(int)

print('X.shape', X.shape)
print('y.shape', y.shape)

torch.manual_seed(42)  # random seed for reproducibility
np.random.seed(42)  # random seed for reproducibility

X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    stratify=y,
                                                    train_size=0.2, # 2k train samples
                                                    test_size=0.05) # 1k test samples
print('X_train.shape',X_train.shape)
print('X_test.shape',X_test.shape)


# to use in code, make datasets from X_train, X_test, y_train, y_test and then pass them to the data loader

# Here, make the unlabelled data set. Set certain number of items in y_train as -1. 90% of samples are unlabelled
# labels_to_drop = np.random.choice(len(y_train),
#                  size=int(len(y_train) * 0.9),
#                  replace=False)
# y_train[labels_to_drop] = -1


# convert to tensors
X_train_tensor = torch.Tensor(X_train)
y_train_tensor = torch.tensor(y_train)
X_test_tensor = torch.Tensor(X_test)
y_test_tensor = torch.tensor(y_test)


# create a dataset
trainset = TensorDataset(X_train_tensor, y_train_tensor)
testset = TensorDataset(X_test_tensor, y_test_tensor)

# create a dataloader
train_loader = torch.utils.data.DataLoader(trainset, batch_size=64)
test_loader = torch.utils.data.DataLoader(testset, batch_size=1)

"""# Define Model Architecture"""

# The paper's model is 
# " Our classifier was a neural network (NN) with one hidden layer consisting of 100 hidden units. We
# used ReLU activation function for hidden units, and used softmax activation function for all the output units. "
# TODO: code the above model. Reviewer: Is the below code correct?

class Net(nn.Module):
    def __init__(self, n_inputs, n_hidden, n_classes):
        super(Net, self).__init__()
        self.input_size = n_inputs
        self.output_size = n_classes
        self.hidden_size = n_hidden
        self.H = n_hidden
        self.fc1 = nn.Linear(n_inputs, n_hidden)
        self.fc2 = nn.Linear(n_hidden, n_classes)
        self.softmax = nn.Softmax(dim=1)
        self.cuda() # move model to cuda

    def add_hidden_units(self, n_new=1):
        self.cpu() # temporarily move model to cpu
        
        self.hidden_size += n_new # update hidden size count

        # take a copy of the current weights stored in self.fcs
        current = [ix.weight.data for ix in [self.fc1, self.fc2]]

        # make the new weights in and out of hidden layer you are adding neurons to
        hl_input = torch.zeros([n_new, current[0].shape[1]])
        nn.init.xavier_uniform_(hl_input)#, gain=nn.init.calculate_gain('relu'))

        hl_output = torch.zeros([current[1].shape[0], n_new])
        nn.init.xavier_uniform_(hl_input)#, gain=nn.init.calculate_gain('relu'))

        # concatenate the old weights with the new weights
        new_wi = torch.cat([current[0], hl_input], dim=0)
        new_wo = torch.cat([current[1], hl_output], dim=1)

        # reset weight and grad variables to new size
        self.fc1 = nn.Linear(current[0].shape[1], self.hidden_size)
        self.fc2 = nn.Linear(self.hidden_size, current[1].shape[0])

        # set the weight data to new values
        self.fc1.weight.data = new_wi.clone().detach().requires_grad_(True)
        self.fc2.weight.data = new_wo.clone().detach().requires_grad_(True)
        
        # move model back to cuda
        self.cuda()

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# print amount of parameters given some n_params value
model = Net(n_inputs=784, n_hidden=1, n_classes=10)
model.add_hidden_units()
print(summary(model, (model.input_size,)))

"""# Define hyperparameters and instantiate VAT model"""

# set hyper params
#n_params_list = [constant*(10**3) for constant in range(4,800,40)] # will be used for experimenting, after sanity checks.
# ^^^ n_parmas_list was inspired by the double descent paper, 20 equally spaced numbers from 4 to 800
n_classes = 10
MNIST_DEFAULT_NUM_INPUTS = 784

# Instantiate model, vat obj and optimizer
def get_model_and_optim(n_inputs=MNIST_DEFAULT_NUM_INPUTS, n_hidden=2, n_classes=10, lr=0.0001,
                        reuse_weights=False, prev_model=None):
  model = None
  if reuse_weights:
    model = prev_model
    model.add_hidden_units()
  else:
    model = Net(n_inputs=n_inputs, n_hidden=n_hidden, n_classes=n_classes) # N = number of params, hardcoded now as 100
    
  vat = shadow.vat.VAT(model=model) # Just giving the model and keeping everything else as defaults
  optimizer = optim.SGD(vat.parameters(), lr=lr)#, momentum=0.95)
  #optimizer = optim.AD(vat.parameters(), lr=lr)#, momentum=0.95)
  print(summary(model, (model.input_size,)))
  return vat, optimizer, model

"""# Train Code"""

def train(vat, optimizer, train_loader, epochs=10):
  xEnt = torch.nn.CrossEntropyLoss(ignore_index=-1) # loss function which ignores the unlabled data
  #xEnt = torch.nn.CrossEntropyLoss()
  vat.to(device) #  moves the model to main memory/RAM
  losscurve = []

  for epoch in range(epochs):
      vat.train() # set the mode of the model.
      lossavg = []

      for i, (data, targets) in enumerate(train_loader):
          x = data.to(device)
          y = targets.to(device)
          optimizer.zero_grad()
          out = vat(x) # this is the model's predication on the current batch
          loss = xEnt(out, y) + vat.get_technique_cost(x) # get_technique_cost is the vat's cost
          loss.backward()
          optimizer.step()
          lossavg.append(loss.item())
          # print('loss item', loss.item())

      losscurve.append(np.mean(lossavg))
      print('epoch {} loss: {}'.format(epoch, losscurve[-1]))
  
  return losscurve[-1]

"""# Test Code"""

def test(vat, test_loader):
  xEnt = torch.nn.CrossEntropyLoss()
  vat.eval()
  y_pred, y_true = [], []
  losses = []

  for i, (data, targets) in enumerate(test_loader):
      x = data.to(device)
      y = targets.to(device)
      out = vat(x)
      y_true.extend(y.detach().cpu().tolist())
      y_pred.extend(torch.argmax(out, 1).detach().cpu().tolist())
      loss = xEnt(out, y)
      losses.append(loss.item())

  test_acc = (np.array(y_true) == np.array(y_pred)).mean() * 100
  print('test accuracy: {}%'.format(test_acc))
  test_loss = np.mean(losses)
  print(losses)
  print('test loss: {}'.format(test_loss))

  return test_loss, test_acc

"""# Exp. 1 (No momentum, 200 epochs)"""

import matplotlib.pyplot as plt
plt.style.use('seaborn-white')
H_list = range(1,41, 2)
train_losses = []
test_losses = []
test_errs = []
N_values = []
prev_model = None

torch.manual_seed(42 * 2)  # random seed for reproducibility
np.random.seed(42 * 2)  # random seed for reproducibility

for H in H_list:
  
  d = 784  # input size
  K = 10  # number of classes
  N = (d+1)*H + (H+1)*K
  N_values.append(N)
  print('# params =', f'{N:,}')
  NEED_TO_REUSE_WEIGHTS = N < 20000 and H > 1
  vat, optimizer, prev_model = get_model_and_optim(n_hidden=H, lr=1e-4,
                                                   reuse_weights=NEED_TO_REUSE_WEIGHTS,
                                                   prev_model=prev_model)
  
  train_loss = train(vat=vat, optimizer=optimizer, train_loader=train_loader, 
                     epochs=200)
  test_loss, test_acc = test(vat=vat, test_loader=test_loader)

  train_losses.append(train_loss)
  test_losses.append(test_loss)
  # test_errs.append(((100-test_acc)/100)**2)
  
  # plot current history
  plt.figure(figsize=(4,3))
  N_values_1000s = np.array(N_values)/(10**3)
  plt.plot(N_values_1000s, train_losses, marker='d', label='Train Loss')
  plt.plot(N_values_1000s, test_losses, marker='d', label='Test Loss')
  # plt.plot(N_values_1000s, test_errs, marker='d', label='Squared Test Error')
  plt.title('Loss vs. Model Size')
  plt.ylabel('Loss')
  plt.xlabel('No. parameters (×10^3)')
  plt.legend(loc="upper right")
  plt.show()

# plot entire history
plt.figure(figsize=(8,6))
N_values_1000s = np.array(N_values)/(10**3)
plt.plot(N_values_1000s, train_losses, marker='d', label='Train Loss')
plt.plot(N_values_1000s, test_losses, marker='d', label='Test Loss')
# plt.plot(N_values_1000s, test_errs, marker='d', label='Squared Test Error')
plt.title('Loss vs. Model Size')
plt.ylabel('Loss')
plt.xlabel('No. parameters (×10^3)')
plt.legend(loc="upper right")
plt.show()

"""# Exp. 2 (same but 500 epochs)"""

import matplotlib.pyplot as plt
plt.style.use('seaborn-white')
H_list = range(1,41, 2)
train_losses = []
test_losses = []
test_errs = []
N_values = []
prev_model = None

torch.manual_seed(42 * 2)  # random seed for reproducibility
np.random.seed(42 * 2)  # random seed for reproducibility

for H in H_list:
  
  d = 784  # input size
  K = 10  # number of classes
  N = (d+1)*H + (H+1)*K
  N_values.append(N)
  print('# params =', f'{N:,}')
  NEED_TO_REUSE_WEIGHTS = N < 20000 and H > 1
  vat, optimizer, prev_model = get_model_and_optim(n_hidden=H, lr=1e-4,
                                                   reuse_weights=NEED_TO_REUSE_WEIGHTS,
                                                   prev_model=prev_model)
  
  train_loss = train(vat=vat, optimizer=optimizer, train_loader=train_loader, 
                     epochs=500)
  test_loss, test_acc = test(vat=vat, test_loader=test_loader)

  train_losses.append(train_loss)
  test_losses.append(test_loss)
  # test_errs.append(((100-test_acc)/100)**2)
  
  # plot current history
  plt.figure(figsize=(4,3))
  N_values_1000s = np.array(N_values)/(10**3)
  plt.plot(N_values_1000s, train_losses, marker='d', label='Train Loss')
  plt.plot(N_values_1000s, test_losses, marker='d', label='Test Loss')
  # plt.plot(N_values_1000s, test_errs, marker='d', label='Squared Test Error')
  plt.title('Loss vs. Model Size')
  plt.ylabel('Loss')
  plt.xlabel('No. parameters (×10^3)')
  plt.legend(loc="upper right")
  plt.show()

# plot entire history
plt.figure(figsize=(8,6))
N_values_1000s = np.array(N_values)/(10**3)
plt.plot(N_values_1000s, train_losses, marker='d', label='Train Loss')
plt.plot(N_values_1000s, test_losses, marker='d', label='Test Loss')
# plt.plot(N_values_1000s, test_errs, marker='d', label='Squared Test Error')
plt.title('Loss vs. Model Size')
plt.ylabel('Loss')
plt.xlabel('No. parameters (×10^3)')
plt.legend(loc="upper right")
plt.show()

"""# Exp. 3 (same but more fine-grained plot)"""

import matplotlib.pyplot as plt
plt.style.use('seaborn-white')
H_list = range(1,61)
train_losses = []
test_losses = []
test_errs = []
N_values = []
prev_model = None

torch.manual_seed(42 * 2)  # random seed for reproducibility
np.random.seed(42 * 2)  # random seed for reproducibility

for H in H_list:
  
  d = 784  # input size
  K = 10  # number of classes
  N = (d+1)*H + (H+1)*K
  N_values.append(N)
  print('# params =', f'{N:,}')
  NEED_TO_REUSE_WEIGHTS = N < 20000 and H > 1
  vat, optimizer, prev_model = get_model_and_optim(n_hidden=H, lr=1e-4,
                                                   reuse_weights=NEED_TO_REUSE_WEIGHTS,
                                                   prev_model=prev_model)
  
  train_loss = train(vat=vat, optimizer=optimizer, train_loader=train_loader, 
                     epochs=500)
  test_loss, test_acc = test(vat=vat, test_loader=test_loader)

  train_losses.append(train_loss)
  test_losses.append(test_loss)
  # test_errs.append(((100-test_acc)/100)**2)
  
  # plot current history
  plt.figure(figsize=(4,3))
  N_values_1000s = np.array(N_values)/(10**3)
  plt.plot(N_values_1000s, train_losses, marker='d', label='Train Loss')
  plt.plot(N_values_1000s, test_losses, marker='d', label='Test Loss')
  # plt.plot(N_values_1000s, test_errs, marker='d', label='Squared Test Error')
  plt.title('Loss vs. Model Size')
  plt.ylabel('Loss')
  plt.xlabel('No. parameters (×10^3)')
  plt.legend(loc="upper right")
  plt.show()

# plot entire history
plt.figure(figsize=(8,6))
N_values_1000s = np.array(N_values)/(10**3)
plt.plot(N_values_1000s, train_losses, marker='d', label='Train Loss')
plt.plot(N_values_1000s, test_losses, marker='d', label='Test Loss')
# plt.plot(N_values_1000s, test_errs, marker='d', label='Squared Test Error')
plt.title('Loss vs. Model Size')
plt.ylabel('Loss')
plt.xlabel('No. parameters (×10^3)')
plt.legend(loc="upper right")
plt.show()

plt.figure(figsize=(6,4.5))
N_values_1000s = np.array(N_values)/(10**3)
plt.plot(N_values_1000s, train_losses, marker='d', label='Train Loss')
plt.plot(N_values_1000s, test_losses, marker='d', label='Test Loss')
plt.title('Loss vs. Model Size')
plt.ylabel('Loss')
plt.xlabel('No. parameters (×10^3)')
plt.legend(loc="upper right")
plt.savefig('sup.png', dpi=300)
plt.show()

import matplotlib
# plot current history
fig1, ax = plt.subplots(figsize=(6,4.5))
N_values_1000s = np.array(N_values)/(10**3)
# plt.xscale('log', basex=10)
# plt.xticks(N_values)
ax.plot(N_values_1000s, train_losses, marker='d', label='Train Loss')
ax.plot(N_values_1000s, test_losses, marker='d', label='Test Loss')
ax.set_title('Loss vs. Model Size')
ax.set_ylabel('Loss')
ax.set_xlabel('No. parameters (×10^3)')
ax.legend(loc="upper right")

ax.set_xscale('log', basex=10)
ax.set_xticks([1, 2, 3, 5, 7, 10, 15, 20, 25, 30, 40, 50])
ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())
plt.savefig('sup2.png', dpi=300)
plt.show()